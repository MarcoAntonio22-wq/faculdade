{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "from spacy.lang.pt.examples import sentences\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessText:\n",
    "    def init(self, model):\n",
    "        self.nlp_model = spacy.load(model)\n",
    "\n",
    "    # def is_date_format(self, text):\n",
    "    #     return (\n",
    "    #         len(text) == 10\n",
    "    #         and text[2] == \"/\"\n",
    "    #         and text[5] == \"/\"\n",
    "    #         and text[:2].isdigit()\n",
    "    #         and text[3:5].isdigit()\n",
    "    #         and text[6:].isdigit()\n",
    "    #     )\n",
    "\n",
    "    def processtext(self, text):\n",
    "        doc = self.nlpmodel(text.lower())\n",
    "\n",
    "        tokens = []\n",
    "        words = []\n",
    "        for token in doc:\n",
    "            words.append(token)\n",
    "            if (\n",
    "                not token.is_stop # Remove stopwords\n",
    "                and not token.ispunct # Remove pontuação\n",
    "                and token.lemma.strip() != \"\" # Remove palavras vazias\n",
    "                and len(token.lemma_.strip()) > 5 # Remove falavras de tamanaho menor que 5\n",
    "            ):\n",
    "                if not (\n",
    "                    any(char.isdigit() for char in token.text)\n",
    "                    # and not self.is_dateformat(token.text)\n",
    "                ):\n",
    "                    tokens.append(unidecode(token.lemma.strip()))\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PreProcessText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = []\n",
    "for i in sentences:\n",
    "    frases.append(pp.processtext(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "frases = []\n",
    "for token in doc:\n",
    "    frases.append(token.text)\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apple está querendo comprar uma startup do Reino Unido por 100 milhões de dólares'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(language='portuguese')\n",
    "topic_model.fit_transform(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicos = topic_model.get_topic_info()\n",
    "topicos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
